name: Daily Forex Data Pipeline

# Run daily at 1 AM UTC to fetch previous day's data
on:
  schedule:
    - cron: '0 1 * * *'  # Daily at 1:00 AM UTC
  
  # Allow manual trigger for testing
  workflow_dispatch:
    inputs:
      date:
        description: 'Specific date to fetch (YYYY-MM-DD)'
        required: false
        type: string
      symbol:
        description: 'Currency pair (e.g., EUR/USD)'
        required: false
        default: 'EUR/USD'
        type: string
      interval:
        description: 'Time interval (e.g., 5min, 1h, 1day)'
        required: false
        default: '5min'
        type: string

jobs:
  extract-and-load:
    name: Extract Forex Data and Upload to GCS
    runs-on: ubuntu-latest
    
    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4
      
      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: ðŸ“¦ Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ðŸ” Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: ðŸ§± Ensure BigQuery dataset and external table
        env:
          GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }}
        run: |
          if ! bq --project_id=${{ secrets.GCP_PROJECT_ID }} show ${{ secrets.GCP_PROJECT_ID }}:forex_analysis >/dev/null 2>&1; then
            bq --project_id=${{ secrets.GCP_PROJECT_ID }} --location=europe-central2 mk -d ${{ secrets.GCP_PROJECT_ID }}:forex_analysis
          fi
          QUERY="CREATE EXTERNAL TABLE IF NOT EXISTS \`${{ secrets.GCP_PROJECT_ID }}.forex_analysis.eurusd_raw\` OPTIONS (format = 'PARQUET', uris = ['gs://${GCS_BUCKET_NAME}/eur_usd/**']);"
          bq --project_id=${{ secrets.GCP_PROJECT_ID }} query --use_legacy_sql=false "$QUERY"
      
      - name: ðŸ“Š Extract Forex Data
        env:
          TWELVE_DATA_API_KEY: ${{ secrets.TWELVE_DATA_API_KEY }}
          GCS_BUCKET_NAME: ${{ secrets.GCS_BUCKET_NAME }}
        run: |
          if [ -n "${{ github.event.inputs.date }}" ]; then
            python scripts/extract_forex.py \
              --symbol "${{ github.event.inputs.symbol }}" \
              --interval "${{ github.event.inputs.interval }}" \
              --date "${{ github.event.inputs.date }}"
          else
            python scripts/extract_forex.py \
              --symbol "EUR/USD" \
              --interval "5min" \
              --lookback-days 1
          fi
      
      - name: âœ… Extraction Complete
        if: success()
        run: echo "âœ… Data successfully extracted and uploaded to GCS"
      
      - name: âŒ Extraction Failed
        if: failure()
        run: |
          echo "âŒ Data extraction failed. Check logs for details."
          exit 1

  run-dbt:
    name: Run dbt Models
    needs: extract-and-load
    runs-on: ubuntu-latest
    
    steps:
      - name: ðŸ“¥ Checkout repository
        uses: actions/checkout@v4
      
      - name: ðŸ Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: ðŸ“¦ Install dbt and dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
      
      - name: ðŸ” Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: ðŸ”§ Configure dbt profile
        run: |
          mkdir -p ~/.dbt
          printf '%s' '${{ secrets.GCP_SA_KEY }}' > /tmp/sa.json
          cat > ~/.dbt/profiles.yml << EOF
          forex_data_pipeline:
            target: prod
            outputs:
              prod:
                type: bigquery
                method: service-account
                project: ${{ secrets.GCP_PROJECT_ID }}
                dataset: forex_analysis
                location: europe-central2
                keyfile: /tmp/sa.json
                threads: 4
                timeout_seconds: 300
          EOF
      
      - name: ðŸ§ª Run dbt debug
        run: dbt debug
      
      - name: ðŸ“¥ Load data to BigQuery
        run: dbt run --select staging.*
      
      - name: ðŸ­ Build marts
        run: dbt run --select marts.*
      
      - name: âœ… dbt Pipeline Complete
        if: success()
        run: echo "âœ… dbt models successfully executed"
      
      - name: âŒ dbt Pipeline Failed
        if: failure()
        run: |
          echo "âŒ dbt pipeline failed. Check logs for details."
          exit 1
